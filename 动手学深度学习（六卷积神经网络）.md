# 卷积神经网络基础

## 特征

> 平移不变性  网络的反应与特征出现的位置无关
>
> 局部性          更多的关注局部区域，不过度关注在一图像中相隔较远区域之间的关系

## 由全连接层推到卷积

* 在全连接层中

输       入  一维度向量 

经过一个  二维矩阵 

> 输入中的每个点受其余所有点（一维）加权影响，该权重用一维数组表述，共需要与输入相匹配个一维数组
>
> 所以是二维矩阵

输        出  二维向量



输        出  一维向量

表示为    
$$
h_{i} = \sum_{k} w_{ik}x_{k}
$$

每个$i$（输入的点）有其对应的一整套 $k$（对应的权重矩阵）  

* 卷积过程中

输       入  二维度向量 

经过一个  四维矩阵 

> 输入中的每个点受其余所有点(二维)加权影响，该权重用二维数组表述，共需要与输入相匹配个二维数组
>
> 所以是四维矩阵

输        出  二维向量

表示为   
$$
h_{i,j}=\sum_{k,l}w_{i,j,k,l}x_{k,l} = \sum_{a,b}v_{i,j,a,b}x_{i+a,j+b}
$$

每个$i,j$（输入的点）有其对应的一整套 $k,l$（对应的权重矩阵） 

此处的对应关系理解为该套 $k,l$（对应的权重矩阵） 是与指定的 $i,j$相关，因此在其上做偏移
$$
v_{i,j,a,b} = w_{i,j,i+a,j+b}
$$


![img](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202408162225449.jpeg)

* 赋予平移不变性

在上式中 $x_{i+a,j+b}$中 $ij$  影响最终结果$h_{i,j}$ （$v_{i,j,a,b}$作为乘数，其值受到影响）并无**平移不变性**  

因此要消去$ij$对 $v$ 的影响

令  $v_{i,j,a,b} = v_{a,b}$       Kern
$$
h_{i,j}=\sum_{a,b}v_{a,b}x_{i+a,j+b}
$$
此时不同位置（具有不同$ij$）相同值的$x$能得到相同的$h$

* 赋予局部性

不关注过多的范围
$$
h_{i,j}=\sum_{a=-\Delta}^{\Delta}\sum_{b=-\Delta}^{\Delta}v_{a,b}x_{i+a,j+b}
$$

## 填充与步长

 ### 填充

正常输出结果$(n_{h}-k_{h}+1)*(n_w-k_w+1)$

会导致输出的结果变小

为了保证输出结果与输入结果大小一致，采用填充

$(n_{h}-k_{h}+p_{h}+1)*(n_w-k_w+p_{w}+1)$

$p_h = k_h-1$

$p_w = k_w-1$

如果 $k$的边长都为奇数，每边填充的值 $(k_h-1)/2 $

如果是偶数，一边多填充一点

### 步幅

 增加每次移动的距离

$(n_{h}-k_{h}+p_{h}+s_h)/s_h*(n_w-k_w+p_{w}+s_w)/s_w$

 ## 通道

输入$X: c_i  *n_h * n_w$

卷积核$W:c_o*c_i*k_h*k_w$

输出$Y: c_o*m_h*m_w$

 1*1 的卷积核等于将输入拉成 $X: c_i  * （n_h * n_w）$的二维向量   卷积核 $K: c_o*c_i$

![image-20240818155352741](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202408181553105.png)

## 池化

滑动窗口，取窗口内值，缓解卷积对位置的敏感

取 最大、平均 忽略不重要的保留较重要的

![image-20241025182148366](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202410251821953.png)