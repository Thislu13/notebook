

# 迁移学习

> 广义上讲， 只要利用我们已有的知识、模型、结构帮助我们实现在目标数据集上的学习目标，这一过程被称为迁移学习

边缘概率 特征相关的概率 $P(X)$

条件概率 特征与类别之间的关系 $P(y|X)$



迁移学习 transfer learning

预训练-微调 Pre-train and fine-tune

领域自适应 Domain adaptation

领域泛化 Domain generalization

## TCA 基于特征的迁移学习方法（transfer component analysis）

11年的IEEE Trans. Neural Network and Learning System

类似于机器学习内的数据降维，通过降维 将数据维度减少（PCA，主成分分析）

在迁移学习的过程中

原始数据域与目标数据域通常处于不同的数据分布

将两个数据域的数据映射到一个高维空间（希尔伯特空间），最小化映射后两个数据集之间的距离
$$
P(X_s)\neq P(X_t)
$$

$$
P(\phi(X_s)\approx P(\phi (X_t)
$$

因此我们需要找到一个好的$\phi$

采用最大均值差异 MMD
$$
dist(X'_{src}, X'_{tar} ) = \left\|\frac{1}{m} \sum_{i=1}^{n_1} \phi(x_{src_{i}}) - \frac{1}{n} \sum_{j=1}^{n_2} \phi(x_{tar_{i}})\right\|_{\mathcal{H}}
$$

$$
\min_A \text{MMD}(A^\top X_s, A^\top X_t) + \lambda \|A\|_F^2
$$

其中，$\lambda$ 是正则化参数，用于控制变换矩阵的复杂度。



优点 

​	简单有效

缺点

​	只考虑了边缘分布，没有考虑条件分布

## JDA 联合分布适配方法（joint distribution adaptation）

TCA的plus版本

在JDA中有两个基础假设

* 源数据 和 目标数据边缘分布不同
* 原数据 和 目标数据条件分布不同

自然的我们要找到其中对应的变换

找一个变换A

$P(A^TX_s)$  让其接近 $P(A^T X_t)$

同时

$P(y_s|A^TX_s)$ 与$P(y_t|A^T X_t)$接近
$$
\min_A \text{MMD}(A^\top X_s, A^\top X_t) + \mu \sum_{c=1}^{C} \text{MMD}(A^\top X_s^c, A^\top X_t^c) + \lambda \|A\|_F^2

$$
其中$ X_s^c$ 表示 第c类标签对应特征

$\mu$ 调节两个分本之间权重关系

$\lambda$ 是正则化参数，用于控制变换矩阵的复杂度。

优点 

​	考虑更加充分

缺点

​	计算复杂度高，如果有标签冲突可能要引入新的方法

* 部分匹配
* 伪标签
* 通过聚类进行对齐	



> 微调加速了收敛
>
> 相似数据不同任务, 不同数据相似任务
>
> 一定程度上有概率提升准确率

用小数据在已经训练好的大数据集模型上微调的方法

pre-train





* 一般直接copy  encoder层，然后在此基础上进行进一步训练

  > 我们认为大模型在特征提取（特征工程上已经做的很好了）泛化能力强

  * 学习率与学习次数调小，因为往往比较接近较好的结果了，同时大模型更复杂，在小数据集上学习次数过多容易造成过拟合

* 将最底层直接冻结

  * 从模型初始位置以递增的方式设置学习率  将最(或最下面几层)层 都设置为0

  > 我们认为深度学习的过程中是循序渐进的,
  >
  > 越往底层(比较初期的层)学的东西越抽象
  >
  > ![image-20240818170423268](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202408181704139.png)
  >
  > 越往上层(比较后期的层)学的东西更符合语义
  >
  > ![image-20240818170533143](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202408181705006.png)

![image.png](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202408231736032.png)
