> 微调加速了收敛
>
> 相似数据不同任务, 不同数据相似任务
>
> 一定程度上有概率提升准确率

用小数据在已经训练好的大数据集模型上微调的方法

pre-train





* 一般直接copy  encoder层，然后在此基础上进行进一步训练

  > 我们认为大模型在特征提取（特征工程上已经做的很好了）泛化能力强

  * 学习率与学习次数调小，因为往往比较接近较好的结果了，同时大模型更复杂，在小数据集上学习次数过多容易造成过拟合

* 将最底层直接冻结

  * 从模型初始位置以递增的方式设置学习率  将最(或最下面几层)层 都设置为0

  > 我们认为深度学习的过程中是循序渐进的,
  >
  > 越往底层(比较初期的层)学的东西越抽象
  >
  > ![image-20240818170423268](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202408181704139.png)
  >
  > 越往上层(比较后期的层)学的东西更符合语义
  >
  > ![image-20240818170533143](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202408181705006.png)

