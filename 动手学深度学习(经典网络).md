# LeNet

![image-20240818200133656](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202408182001105.png)

# AlexNet

> 认为模型在最底层学习到了一些类似于传统滤波器的特征提取器
>
> ![image-20240822190059487](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202408221901151.png)

模型架构

![image-20240822190137219](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202408221901673.png)

# VGG（使用块的网络）

块的基本组成

1.带填充保持图片尺寸的卷积

2.非线性激活函数

3.汇聚层，缩小图片尺寸

```python
import torch
from torch import nn
from d2l import torch as d2l

def vgg_block(num_convs, in_channels, out_channels):
    layers = []
    for _ in range(num_convs):
        layers.append(nn.Conv2d(in_channels, out_channels,
                                kernel_size=3, padding=1))
        layers.append(nn.ReLU())
        in_channels = out_channels
    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))
    return nn.Sequential(*layers)
```

![image-20240829191735916](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202408291917112.png)

# NiN块

```python
def nin_block(in_channels, out_channels, kernel_size, stride, padding):
    layer = []
    layer.append(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, stride=stride))
    layer.append(nn.ReLU())
    layer.append(nn.Conv2d(out_channels, out_channels, kernel_size=1))
    layer.append(nn.ReLU())
    layer.append(nn.Conv2d(out_channels, out_channels, kernel_size=1))
    layer.append(nn.ReLU())

    return nn.Sequential(*layer)
```

# GoogLeNet

![image-20241024193223922](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202410241932993.png)

**inception块**

大量增加卷积类型，卷积次数但同时降低卷积运算数量，参数大小

![image-20241025192837967](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202410251928009.png)

后续升级：

v2 :加入 batch-normalization

v3 :

* 将5x5 替换为 多个 3x3
* 将 3x3替换为 1x7和7x1
* 替换3x3为 1x3和3x1（并行）
* 加深

# ResNet

## residua block

![image-20241031195410443](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202410311954533.png)

> 右边虚线框内希望学习到的是 $f(x) - x$
>
> 该块可定义为 $g(x) = f(x) + x$ 此处$f(x)$指的是虚线块内，区别于上面的 $f(x)$

```python
class Residual(nn.Module):
    def __init__(self, input_channels, num_channels, use_1x1conv=False, strides=1):
        super().__init__()
        self.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1, stride=strides)
        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)
        if use_1x1conv:
            self.conv3 = nn.Conv2d(input_channels, num_channels, kernel_size=1, stride=strides)
        else:
            self.conv3 = None

        self.bn1 = nn.BatchNorm2d(num_channels)
        self.bn2 = nn.BatchNorm2d(num_channels)
    def forward(self, X):
        Y = F.relu(self.bn1(self.conv1(X)))
        Y = self.bn2(self.conv1(Y))
        if self.conv3:
            X = self.conv3(X)
        Y += X

        return F.relu(Y)
```

residual_block 有两种模式

* input_channels == output_channels 并且 图片（特征）的尺寸未发生改变，在该种情况下，分出去的部分尺寸与经过两层卷积之后结构的尺寸保持一致，这种情况下分出去的部分不需要做多余的操作，直接与两层卷积得到的结果相加就可以
* input_channels ！= output_channels 或者 图片（特征）的尺寸发生改变， 在该种情况下，分出去部分的尺寸与经过两次卷积之后结果的尺寸不再一致，需要通过1x1的卷积核，卷积核的输出控channels，卷积核的strides控制尺寸

在实际模型中，

除了第一个resnet_block块是由两个不改变大小的residual模块，

其余resnet_block块均由一个大小减半通道翻倍的residual模块，外加一个不改变大小的residual模块组成

后续版本中将 **‘卷积层、批量规范化层、激活层’ **更改为 **‘批量规范化层、激活层、卷积层‘**

![image-20241111143034329](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202411111430735.png)

# DenseNet

在resnet中我们将函数展开为

$$f(x) = x+g(x)$$

在densenet中我们强化这种思想，将函数展开的更为复杂，拆成更多的项

![image-20241111160907046](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202411111609097.png)

区别在于resnet中支路与主干直接进行相加操作，而在densenet中采用了沿通道方向拼接的方法。

最小单元块使用改良版resnet **“批量归一化、激活、卷积”**
```python
def conv_block(input_channels, num_channels):
    return nn.Sequential(
        nn.BatchNorm2d(input_channels),
        nn.ReLU(),
        nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1)
    )
```

一个dense_block由多个卷积块组成，输入的通道数随层数增加（不断累加）
```python
class DenseBlock(nn.Module):
    def __init__(self, num_convs, input_channels, num_channels):
        super(DenseBlock, self).__init__()
        layer = []
        for i in range(num_convs):
            layer.append(conv_block(num_channels*i + input_channels, num_channels))
        self.net = nn.Sequential(*layer)
        
    def forward(self, X):
        for blk in self.net:
            Y = blk(X)
            X = torch.cat((X,Y), dim=1)
        return X
```

在dense _block块中，通道数由较多增加，同时图片尺寸并未减小，因此我们引入过渡层 transition_block 
```python
def transition_block(input_channels, num_channels):
   return nn.Sequential(
           nn.BatchNorm2d(input_channels),
           nn.ReLU(),
           nn.Conv2d(input_channels, num_channels,kernel_size=1),
           nn.AvgPool2d(kernel_size=2, stride=2)
        )
```

