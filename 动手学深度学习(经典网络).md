# LeNet

![image-20240818200133656](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202408182001105.png)

# AlexNet

> 认为模型在最底层学习到了一些类似于传统滤波器的特征提取器
>
> ![image-20240822190059487](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202408221901151.png)

模型架构

![image-20240822190137219](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202408221901673.png)

# VGG（使用块的网络）

块的基本组成

1.带填充保持图片尺寸的卷积

2.非线性激活函数

3.汇聚层，缩小图片尺寸

```python
import torch
from torch import nn
from d2l import torch as d2l

def vgg_block(num_convs, in_channels, out_channels):
    layers = []
    for _ in range(num_convs):
        layers.append(nn.Conv2d(in_channels, out_channels,
                                kernel_size=3, padding=1))
        layers.append(nn.ReLU())
        in_channels = out_channels
    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))
    return nn.Sequential(*layers)
```

![image-20240829191735916](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202408291917112.png)

# NiN块

```python
def nin_block(in_channels, out_channels, kernel_size, stride, padding):
    layer = []
    layer.append(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, stride=stride))
    layer.append(nn.ReLU())
    layer.append(nn.Conv2d(out_channels, out_channels, kernel_size=1))
    layer.append(nn.ReLU())
    layer.append(nn.Conv2d(out_channels, out_channels, kernel_size=1))
    layer.append(nn.ReLU())

    return nn.Sequential(*layer)
```

# GoogLeNet

![image-20241024193223922](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202410241932993.png)

**inception块**

大量增加卷积类型，卷积次数但同时降低卷积运算数量，参数大小