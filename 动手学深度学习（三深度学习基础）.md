#深度学习/基础 
# softmax回归

主要解决分类问题
$$
\hat{y_{1}},\hat{y_{2}},\hat{y_{3}}=softmax(\omicron_{1},\omicron_{2},\omicron_{3})
$$



$$
\hat{y_{1}} = \frac{exp(\omicron_{1})}{\sum^{m}_{i=1}exp(\omicron_{i})} \qquad \hat{y_{2}} = \frac{exp(\omicron_{2})}{\sum^{m}_{i=1}exp(\omicron_{i})}
$$

* 保证转化为和为1的概率分布
* 不改变预测输出

$$
\operatorname*{argman}_i\omicron_i =  \operatorname*{argmax}_i \hat{y_i}
$$

* 自然常数e的指数，保证其非负的属性

![image-20240724220445970](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202407242204892.png)

```python
def softmax(X):
    # 行数等于batch_size 列数等于分类的种类
	X_exp = np.exp(X)
	partition = X_exp.sum(1, keepdims=True)
	return X_exp / partition
```



# 交叉熵损失函数

用 softmax 的值 与 one-hot 编码进行逼近

* 不采用平方损失函数的原因 

  实际上无需使模型其余标签全为零，仅仅需要正确的标签值最大即可

交叉熵（cross entropy）
$$
H(y^{(i)},\hat{y}^{i}) = -\sum^q_{j=1}y^{(i)}_{j}log\hat{y}^{(i)}_j
$$
只有正确的编码 对应的交叉熵损失函数才有值，只关心正确部分

```python
def cross_entropy(y_hat, y):
	return - torch.log(y_hat[range(len(y_hat), y])
```

![image-20240803145111518](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202408031451361.png)

```python
nn.CrossEntropyLoss(reduction='')
```

> 参数 `reduction`
>
> `reduction` 参数用于指定如何处理计算得到的每个样本的损失。它有三个可选值：
>
> - `'none'`: 不对损失进行任何归约，返回一个与输入大小相同的张量。
> - `'mean'`: 计算所有元素损失的均值。
> - `'sum'`: 计算所有元素损失的总和。

# 分类准确率

`y_hat` 是推测得到的 n * m 矩阵 

> 其中 n 是每个批次的数据量 batch_size
>
> 其中 m 是需要分类的类别

```python
def accuracy(y_hat, y):
	if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
		y_hat = y_hat.argmax(axis=1)
	
	cmp = (y.hat.type(y.dtype) == y)
	return float(cmp.typr(y.dtype).sum())/len(y)
```

# 激活函数



## ReLU

$$
ReLU(x) = max(x, 0)
$$

```python
torch.relu()
```

![image-20240803201853465](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202408032018293.png)

![image-20240803202000976](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202408032020588.png)

## sigmoid

$$
sigmoid(x) = \frac{1}{1+exp(-x)}
$$

```python
torch.sigmoid()
```

![image-20240803202513273](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202408032025900.png)

![image-20240803202539453](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202408032025676.png)

## tanh

$$
tanH(x) = \frac{1-exp(-2x)}{1+exp(-2x))}
$$

```python
torch.tanh
```

![image-20240803203022991](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202408032030683.png)

![image-20240803203040546](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202408032030650.png)

# 权重衰退(正则化)

>  使用限制参数值的选择范围来控制模型容量

$$
||x||^2 \leq \theta
$$

实际中常常在损失函数中加入一个  **罚**（拉格朗日得到）
$$
+\frac{\lambda}{2}||W||^2
$$
计算梯度
$$
\frac{\partial (L(W,b)+\frac{\lambda}{2}||W||^2)}{\partial W} = \frac{\partial L(W,b)}{\partial W} + \lambda W
$$
考虑到参数随时间更新 $$W_{t+1}$$ 的值
$$
W_{t+1} = W_{t} - \eta(\frac{\partial L(W_{t},b)}{\partial W_{t}}+ \lambda W_{t})\\=(1-\eta\lambda)W_{t} - \eta\frac{\partial L(W_{t},b)}{\partial W_{t}}
$$
通常 $$\eta\lambda<1$$  因此又称权重衰退

# Drop out

> 常用于全连接层

 随机的舍弃掉上一层的一些输出，让本层的权重无法发挥作用
$$
x^{'}_{i}\left\{
\begin{aligned}
x & = & 0 &&with&&probablity&&p\\
y & = & \frac{x_{i}}{1-p}&&otherrise\\
\end{aligned}
\right.
$$
抛弃一些，将剩余的放大

![image-20240811162219238](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202408111622227.png)
