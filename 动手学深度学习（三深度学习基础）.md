# softmax回归

主要解决分类问题
$$
\hat{y_{1}},\hat{y_{2}},\hat{y_{3}}=softmax(\omicron_{1},\omicron_{2},\omicron_{3})
$$



$$
\hat{y_{1}} = \frac{exp(\omicron_{1})}{\sum^{m}_{i=1}exp(\omicron_{i})} \qquad \hat{y_{2}} = \frac{exp(\omicron_{2})}{\sum^{m}_{i=1}exp(\omicron_{i})}
$$
保证转化为和为1的概率分布
$$
\operatorname*{argman}_i\omicron_i =  \operatorname*{argmax}_i \hat{y_i}
$$
不改变预测输出



# 交叉熵损失函数

用 softmax 的值 与 one-hot 编码进行逼近

* 不采用平方损失函数的原因 

  实际上无需使模型其余标签全为零，仅仅需要正确的标签值最大即可

交叉熵（cross entropy）
$$
H(y^{(i)},\hat{y}^{i}) = -\sum^q_{j=1}y^{(i)}_{j}log\hat{y}^{(i)}_j
$$
只有正确的编码 对应的交叉熵损失函数才有值，只关心正确部分

