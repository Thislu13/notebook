# softmax回归

主要解决分类问题
$$
\hat{y_{1}},\hat{y_{2}},\hat{y_{3}}=softmax(\omicron_{1},\omicron_{2},\omicron_{3})
$$



$$
\hat{y_{1}} = \frac{exp(\omicron_{1})}{\sum^{m}_{i=1}exp(\omicron_{i})} \qquad \hat{y_{2}} = \frac{exp(\omicron_{2})}{\sum^{m}_{i=1}exp(\omicron_{i})}
$$

* 保证转化为和为1的概率分布
* 不改变预测输出

$$
\operatorname*{argman}_i\omicron_i =  \operatorname*{argmax}_i \hat{y_i}
$$

* 自然常数e的指数，保证其非负的属性

![image-20240724220445970](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202407242204892.png)

```python
def softmax(X):
    # 行数等于batch_size 列数等于分类的种类
	X_exp = np.exp(X)
	partition = X_exp.sum(1, keepdims=True)
	return X_exp / partition
```



# 交叉熵损失函数

用 softmax 的值 与 one-hot 编码进行逼近

* 不采用平方损失函数的原因 

  实际上无需使模型其余标签全为零，仅仅需要正确的标签值最大即可

交叉熵（cross entropy）
$$
H(y^{(i)},\hat{y}^{i}) = -\sum^q_{j=1}y^{(i)}_{j}log\hat{y}^{(i)}_j
$$
只有正确的编码 对应的交叉熵损失函数才有值，只关心正确部分

