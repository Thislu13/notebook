#深度学习/基础 
# softmax回归

主要解决分类问题
$$
\hat{y_{1}},\hat{y_{2}},\hat{y_{3}}=softmax(\omicron_{1},\omicron_{2},\omicron_{3})
$$



$$
\hat{y_{1}} = \frac{exp(\omicron_{1})}{\sum^{m}_{i=1}exp(\omicron_{i})} \qquad \hat{y_{2}} = \frac{exp(\omicron_{2})}{\sum^{m}_{i=1}exp(\omicron_{i})}
$$

* 保证转化为和为1的概率分布
* 不改变预测输出

$$
\operatorname*{argman}_i\omicron_i =  \operatorname*{argmax}_i \hat{y_i}
$$

* 自然常数e的指数，保证其非负的属性

![image-20240724220445970](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202407242204892.png)

```python
def softmax(X):
    # 行数等于batch_size 列数等于分类的种类
	X_exp = np.exp(X)
	partition = X_exp.sum(1, keepdims=True)
	return X_exp / partition
```



# 交叉熵损失函数

用 softmax 的值 与 one-hot 编码进行逼近

* 不采用平方损失函数的原因 

  实际上无需使模型其余标签全为零，仅仅需要正确的标签值最大即可

交叉熵（cross entropy）
$$
H(y^{(i)},\hat{y}^{i}) = -\sum^q_{j=1}y^{(i)}_{j}log\hat{y}^{(i)}_j
$$
只有正确的编码 对应的交叉熵损失函数才有值，只关心正确部分

```python
def cross_entropy(y_hat, y):
	return - torch.log(y_hat[range(len(y_hat), y])
```

![image-20240803145111518](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202408031451361.png)

# 分类准确率

`y_hat` 是推测得到的 n * m 矩阵 

> 其中 n 是每个批次的数据量 batch_size
>
> 其中 m 是需要分类的类别

```python
def accuracy(y_hat, y):
	if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
		y_hat = y_hat.argmax(axis=1)
	
	cmp = (y.hat.type(y.dtype) == y)
	return float(cmp.typr(y.dtype).sum())/len(y)
```

