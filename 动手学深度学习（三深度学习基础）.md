
#深度学习/基础 
# softmax回归

主要解决分类问题
$$
\hat{y_{1}},\hat{y_{2}},\hat{y_{3}}=softmax(\omicron_{1},\omicron_{2},\omicron_{3})
$$



$$
\hat{y_{1}} = \frac{exp(\omicron_{1})}{\sum^{m}_{i=1}exp(\omicron_{i})} \qquad \hat{y_{2}} = \frac{exp(\omicron_{2})}{\sum^{m}_{i=1}exp(\omicron_{i})}
$$

* 保证转化为和为1的概率分布
* 不改变预测输出

$$
\operatorname*{argman}_i\omicron_i =  \operatorname*{argmax}_i \hat{y_i}
$$

* 自然常数e的指数，保证其非负的属性

![image-20240724220445970](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202407242204892.png)

```python
def softmax(X):
    # 行数等于batch_size 列数等于分类的种类
	X_exp = np.exp(X)
	partition = X_exp.sum(1, keepdims=True)
	return X_exp / partition
```



# 交叉熵损失函数

用 softmax 的值 与 one-hot 编码进行逼近

* 不采用平方损失函数的原因 

  实际上无需使模型其余标签全为零，仅仅需要正确的标签值最大即可

交叉熵（cross entropy）
$$
H(y^{(i)},\hat{y}^{i}) = -\sum^q_{j=1}y^{(i)}_{j}log\hat{y}^{(i)}_j
$$
只有正确的编码 对应的交叉熵损失函数才有值，只关心正确部分

```python
def cross_entropy(y_hat, y):
	return - torch.log(y_hat[range(len(y_hat), y])
```

![image-20240803145111518](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202408031451361.png)

```python
nn.CrossEntropyLoss(reduction='')
```

> 参数 `reduction`
>
> `reduction` 参数用于指定如何处理计算得到的每个样本的损失。它有三个可选值：
>
> - `'none'`: 不对损失进行任何归约，返回一个与输入大小相同的张量。
> - `'mean'`: 计算所有元素损失的均值。
> - `'sum'`: 计算所有元素损失的总和。

# 分类准确率

`y_hat` 是推测得到的 n * m 矩阵 

> 其中 n 是每个批次的数据量 batch_size
>
> 其中 m 是需要分类的类别

```python
def accuracy(y_hat, y):
	if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
		y_hat = y_hat.argmax(axis=1)
	
	cmp = (y.hat.type(y.dtype) == y)
	return float(cmp.typr(y.dtype).sum())/len(y)
```

# 激活函数



## ReLU

$$
ReLU(x) = max(x, 0)
$$

```python
torch.relu()
```

![image-20240803201853465](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202408032018293.png)

![image-20240803202000976](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202408032020588.png)

## sigmoid

$$
sigmoid(x) = \frac{1}{1+exp(-x)}
$$

```python
torch.sigmoid()
```

![image-20240803202513273](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202408032025900.png)

![image-20240803202539453](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202408032025676.png)

## tanh

$$
tanH(x) = \frac{1-exp(-2x)}{1+exp(-2x))}
$$

```python
torch.tanh
```

![image-20240803203022991](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202408032030683.png)

![image-20240803203040546](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202408032030650.png)

# 权重衰退(正则化)

>  使用限制参数值的选择范围来控制模型容量

$$
||x||^2 \leq \theta
$$

实际中常常在损失函数中加入一个  **罚**（拉格朗日得到）
$$
+\frac{\lambda}{2}||W||^2
$$
计算梯度
$$
\frac{\partial (L(W,b)+\frac{\lambda}{2}||W||^2)}{\partial W} = \frac{\partial L(W,b)}{\partial W} + \lambda W
$$
考虑到参数随时间更新 $$W_{t+1}$$ 的值
$$
W_{t+1} = W_{t} - \eta(\frac{\partial L(W_{t},b)}{\partial W_{t}}+ \lambda W_{t})\\=(1-\eta\lambda)W_{t} - \eta\frac{\partial L(W_{t},b)}{\partial W_{t}}
$$
通常 $$\eta\lambda<1$$  因此又称权重衰退

# Dropout

> 常用于全连接层

 随机的舍弃掉上一层的一些输出，让本层的权重无法发挥作用
$$
x^{'}_{i}\left\{
\begin{aligned}
x & = & 0 &&with&&probablity&&p\\
y & = & \frac{x_{i}}{1-p}&&otherrise\\
\end{aligned}
\right.
$$
抛弃一些，将剩余的放大

![image-20240811162219238](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202408111622227.png)

# Batch Normal 规范化、批归一化

> 1. 符合标准化（正态分布，均值为1，方差为0）的数据是优雅的 将参数的数量级进行统一
> 2. 在神经网络中，随着层数增加可能会出现更加广泛变化的权重 如果一层网络的权重是另外一层的若干倍，理论上对应的学习率也要有对应的调整。 因此变化范围过大的参数对于我们来说是不友好的
> 3. 层数越多，越复杂，容易过拟合
> 4. 如果每一层的参数相对稳定，我们可以使用更大的学习率，使得学习加快

批量规范化公式
$$
BN(X) = \gamma \cdot\frac{X-\hat{\mu}_{B}}{\hat{\sigma}_{B}} + \beta
$$
>  $\frac{X-\hat{\mu}_{B}}{\hat{\sigma}_{B}}$在这个过程中数据的分布变为正态分布（均值为1、方差为0）
>
> 简易证明如下:
>
> $Z_i = \frac{x_i - \mu}{\sigma}$
>
> $\mu = \frac{1}{n} \sum_{i=1}^{n} x_i$
>
> $\sigma = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_i - \mu)^2}$
>
> 方差的定义为：$ \text{Var}(Z) = \frac{1}{n} \sum_{i=1}^{n} (Z_i - \bar{Z})^2$ 
>
> 其中 $\bar{Z} $ 是 \( Z \) 的均值
>
> 因为我们在标准化时减去了均值 $\mu $，所以 $\bar{Z}$为0： 
>
> $\text{Var}(Z) = \frac{1}{n} \sum_{i=1}^{n} \left( \frac{x_i - \mu}{\sigma} \right)^2$
>
> 这可以展开为： 
>
> $\text{Var}(Z) = \frac{1}{n} \sum_{i=1}^{n} \frac{(x_i - \mu)^2}{\sigma^2}$
>
> 接下来，我们可以将 $\frac{1}{\sigma^2}$
>
> 提到求和符号外面：
>
> $\text{Var}(Z) = \frac{1}{\sigma^2} \cdot \frac{1}{n} \sum_{i=1}^{n} (x_i - \mu)^2$ 
>
> 根据标准差的定义，
>
> $\frac{1}{n} \sum_{i=1}^{n} (x_i - \mu)^2 = \sigma^2$，
>
> 因此我们可以进一步简化： 
>
> $\text{Var}(Z) = \frac{1}{\sigma^2} \cdot \sigma^2 = 1$

其中

$\hat{\mu}_{B}$ 是该批数据（权重）的平均值

$\hat{\sigma}_{B}^{2}$是 这里引入$\varepsilon$确保除数大于0

![image-20240910215733394](https://raw.githubusercontent.com/Thislu13/image_save/main/notebook/202409102157590.png)
