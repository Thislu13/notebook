# Tansformer

> 本质是加权求和     编码和解码是基础

![image-20240710210713909](https://cdn.jsdelivr.net/gh/Thislu13/image_save@main/notebook/202407102107881.png)

![技术发展](https://cdn.jsdelivr.net/gh/Thislu13/image_save@main/notebook/202407102110894.png)



* 图片里的点是并行的   语言是有先后关系的   同时也是 RNN 与 CNN 区别

* RNN解决 seq2seq 机器翻译输入输出长短不同

* 根据单词的上下文关系来找到单词的意思（计算机无法理解现实中的示例）

  > 例如香蕉的上下文 与 猴子 黄色 甜高度相关

* 将词义通过一个高纬度空间展示出来 （one-hat 与 分词器对token的两个极端 ）
  * 数字化
  * 距离有实际含义   
    * one-hat（过于稀疏） 
      * 所有token都是正交的很难标识其关系
    * 分词器（过于稠密） 
      * 在两个词关联时候容易与第三个冲突
      * 有相关的词可能实际距离较远

* 找到一个**潜空间**

* 向量与矩阵相乘一种理解方式是 空间变换（映射关系） 是线性变换（旋转、拉伸）存在一一对应关系

  * 可以认为是向量本身的改变 维度不变的情况下

  * 可以认为产生变换的是矩阵空间，向量在每个维度的分向量（投影）经过矩阵变换（乘以对应的值）

    再将相乘之后得到的结果相加（向量的合成）得到原来的向量

    ![image-20240710215233559](http://cdn.jsdelivr.net/gh/Thislu13/image_save@main/notebook/202407102152474.png)

  * 也可以将上面结合认为同时发生改变

![三种变化](http://cdn.jsdelivr.net/gh/Thislu13/image_save@main/notebook/202407102152540.png)

* 二次型可以认为是 平方操作

$$
P(x) = x^TAx \approx f(x) = ax^2
$$

![image-20240710215835678](http://cdn.jsdelivr.net/gh/Thislu13/image_save@main/notebook/202407102158068.png)

* 以上可知在矩阵乘法的过程中 
  * 第一个数代表 多个向量集合  
  * 第二个数代表 变换的规则(流处理，类比于函数、一维可认为是系数)   
  * 值是变换的结果
  * 因此矩阵不存在交换律
  * 因为是点与点的映射 可能会保留原来的一定的特性（比如向量与向量之间的倍率关系、平行）